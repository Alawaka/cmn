{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_hidden_dim = 512\n",
    "lstm_units = 512\n",
    "classifier_hidden_dim = 256\n",
    "\n",
    "bow_dim = 2632\n",
    "visfeat_dim = 4096+8\n",
    "T = 15\n",
    "\n",
    "weight_decay = 5e-4\n",
    "max_epoches = 10000\n",
    "\n",
    "#pos_weight = 0.5 / 0.05\n",
    "#neg_weight = 0.5 / 0.95\n",
    "margin = 1\n",
    "\n",
    "data_filter = '../exp-txt-classifier/data/training_batches/*/*.npz'\n",
    "\n",
    "exp_name = 'explanation_ranker3_different_class_no_flip'\n",
    "save_dir = './tfmodel/%s/' % exp_name\n",
    "log_dir = './tb/%s/' % exp_name\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explanation_classification_model(bow_batch, visfeat_batch, bbox_score_batch, seq_length_batch,\n",
    "    feature_hidden_dim, lstm_units, classifier_hidden_dim,\n",
    "    scope='explanation_classifier', reuse=None):\n",
    "    \n",
    "    # concatenate all the features, and\n",
    "    # use a fully-connected layer to map the features to a new dimension\n",
    "    all_features = tf.concat([bow_batch, visfeat_batch, bbox_score_batch], axis=-1)\n",
    "    all_features = tf.reshape(all_features, [-1, all_features.get_shape().as_list()[-1]])\n",
    "    all_features_mapped = tf.layers.dense(all_features, feature_hidden_dim, activation=tf.nn.relu)\n",
    "    all_features_mapped = tf.reshape(all_features_mapped, [T, -1, feature_hidden_dim])\n",
    "    \n",
    "    # feed the features into a LSTM\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_units)\n",
    "    _, state = tf.nn.dynamic_rnn(cell, all_features_mapped, sequence_length=seq_length_batch,\n",
    "                             dtype=tf.float32, time_major=True)\n",
    "    \n",
    "    # the final classifier: a two-layer network\n",
    "    embeddings = state.h\n",
    "    embeddings_mapped = tf.layers.dense(embeddings, classifier_hidden_dim, activation=tf.nn.relu)\n",
    "    scores = tf.layers.dense(embeddings_mapped, 1)\n",
    "    scores = tf.reshape(scores, [-1])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the model\n",
    "seq_length_batch = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "bow_batch = tf.placeholder(tf.float32, [T, None, bow_dim])\n",
    "visfeat_batch = tf.placeholder(tf.float32, [T, None, visfeat_dim])\n",
    "bbox_score_batch = tf.placeholder(tf.float32, [T, None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = explanation_classification_model(bow_batch, visfeat_batch,\n",
    "    bbox_score_batch, seq_length_batch, feature_hidden_dim, lstm_units, classifier_hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ranking loss\n",
    "# The scores can be reshaped to (N/2, 2), where in the last dimension 2, the\n",
    "# first one is the positive explanation and the second one is the negative explanation\n",
    "scores_reshaped = tf.reshape(scores, [-1, 2])\n",
    "scores_pos = scores_reshaped[..., 0]\n",
    "scores_neg = scores_reshaped[..., 1]\n",
    "loss_rank = tf.reduce_mean(tf.nn.relu(scores_neg - scores_pos + margin))\n",
    "\n",
    "# Regularization loss\n",
    "regularization_vars = [v for v in tf.trainable_variables()\n",
    "                       if v.op.name.endswith('kernel') or v.op.name.endswith('weights')]\n",
    "print('variables for regularization:')\n",
    "for v in regularization_vars:\n",
    "    print(v.op.name)\n",
    "loss_reg = weight_decay*tf.add_n([tf.nn.l2_loss(v) for v in regularization_vars])\n",
    "\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss_rank + loss_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorboard logging\n",
    "log_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "tf.summary.scalar(\"loss_rank\", loss_rank)\n",
    "log_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data list\n",
    "filelist = glob(data_filter)\n",
    "np.random.shuffle(filelist)\n",
    "\n",
    "num_batch = len(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save snapshot\n",
    "save_path = os.path.join(save_dir, '%08d' % 0)\n",
    "saver.save(sess, save_path)\n",
    "print('Model saved to %s' % save_path)\n",
    "\n",
    "total_pos = 0\n",
    "total_all = 0\n",
    "for n_epoch in range(max_epoches):\n",
    "    for n_batch in range(num_batch):\n",
    "        n_iter = n_epoch*num_batch + n_batch\n",
    "        \n",
    "        batch = dict(np.load(filelist[n_batch]))\n",
    "        \n",
    "        # Randomly sample a subset of negative instances\n",
    "        labels = batch['label_batch'].copy()\n",
    "        labels_pos = np.nonzero(labels)[0]\n",
    "        labels_neg = np.nonzero(~labels)[0]\n",
    "        labels_neg = np.random.choice(\n",
    "            labels_neg, size=len(labels_pos), replace=False)\n",
    "        labels_new = np.concatenate((\n",
    "                labels_pos.reshape((-1, 1)),\n",
    "                labels_neg.reshape((-1, 1))), axis=-1).reshape(-1)\n",
    "        batch['seq_length_batch'] = batch['seq_length_batch'][labels_new]\n",
    "        batch['label_batch'] = batch['label_batch'][labels_new]\n",
    "        batch['bow_batch'] = batch['bow_batch'][:, labels_new, :]\n",
    "        batch['visfeat_batch'] = batch['visfeat_batch'][:, labels_new, :]\n",
    "        batch['bbox_score_batch'] = batch['bbox_score_batch'][:, labels_new, :]\n",
    "        \n",
    "        print('batch size:', len(batch['seq_length_batch']))\n",
    "        total_pos += np.sum(batch['label_batch'])\n",
    "        total_all += batch['label_batch'].size\n",
    "        print('pos ratio: %f = %d / %d' % (total_pos / total_all, total_pos, total_all))\n",
    "        \n",
    "        # just to check the label format\n",
    "        labels = batch['label_batch'].reshape((-1, 2))\n",
    "        assert(np.all(labels[..., 0]))\n",
    "        assert(np.all(~labels[..., 1]))\n",
    "        _, summary = sess.run((train_op, log_op),\n",
    "                              {seq_length_batch: batch['seq_length_batch'],\n",
    "                               bow_batch: batch['bow_batch'],\n",
    "                               visfeat_batch: batch['visfeat_batch'],\n",
    "                               bbox_score_batch: batch['bbox_score_batch']})\n",
    "        print('epoch = %d, batch = %d / %d' % (n_epoch, n_batch, num_batch))\n",
    "        log_writer.add_summary(summary, n_iter)\n",
    "\n",
    "    # save snapshot\n",
    "    save_path = os.path.join(save_dir, '%08d' % (n_epoch+1))\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model saved to %s' % save_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
